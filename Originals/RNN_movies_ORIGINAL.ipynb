{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run again!\n",
    "#%pip install scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36af5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!py -m pip install transformers\n",
    "#!py -m pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db62618",
   "metadata": {},
   "source": [
    "Consider re-running it to truly install every requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!py -m pip cache purge\n",
    "!py -m pip install transformers torchvision torch\"\"\"\n",
    "%pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b67dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "\n",
    "import threading\n",
    "import ast # because the id list is in string form\n",
    "import pandas as pd\n",
    "\n",
    "from Dependencies.AdditionalFunctions import topK_one_hot, smooth_multi_hot\n",
    "\n",
    "from Dependencies.MovieDataset import MovieGenresDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74bf09",
   "metadata": {},
   "source": [
    "### Initialize RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a080ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dependencies.RNN_model_class import RNN\n",
    "my_rnn = RNN()\n",
    "\n",
    "if torch.cuda.is_available() : device = \"cuda:0\"\n",
    "else : device = \"cpu\"\n",
    "my_rnn = RNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e624f5",
   "metadata": {},
   "source": [
    "### Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e332b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_ds = MovieGenresDataset()\n",
    "movie_genre_ds= mgd_ds.getDs() #The entire ds = {title:description:classes}\n",
    "movie_id_loc =mgd_ds.get_classes() #Indicates the class location in the classifier\n",
    "genre_ids = movie_genre_ds['genre_ids'].map(ast.literal_eval).tolist() #The actual list of the genre_ids (was a list of strings representing lists)\n",
    "\n",
    "global pad_value #the value set for padding sequences\n",
    "pad_value=5555"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac855",
   "metadata": {},
   "source": [
    "### **Training Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483b027",
   "metadata": {},
   "source": [
    "Epoch Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddbf3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "clear_event = threading.Event()\n",
    "\n",
    "def subthread_func(iteration):\n",
    "    time.sleep(0.5)\n",
    "    print(f\"Subthread running... Current Iteration: {iteration}\")\n",
    "\n",
    "\n",
    "def epoch_train(rnn, optimizer, dev, train_ds, batch_size=16):\n",
    "    loss_arr = [] #a list that stores the gradient squared\n",
    "    l1_grad_sq = [] #a list that stores the loss of the first rnn layer\n",
    "    l2_grad_sq = [] #a list that stores the loss of the second rnn layer\n",
    "\n",
    "\n",
    "    #--------------------TRAINING LOOP--------------------\n",
    "    i=0\n",
    "    for movie_ovw_batch, target_batch in train_ds:\n",
    "        #outputs = rnn.tokenize_input(movie, dev)\n",
    "        #target = torch.tensor([t for t in target[target!=5555]]).detach() #Padding value is set to be 5555, change accordingly\n",
    "        avg_loss = 0\n",
    "        i+=1\n",
    "        for movie_ovw, target in zip(movie_ovw_batch, target_batch):\n",
    "            classes = torch.tensor(topK_one_hot(target,19)).detach()\n",
    "            # Get the smoothed distribution, for using a normalized probability distribution as the target\n",
    "\n",
    "            y_hat = rnn.forward(movie_ovw.to(dev))# Forward Propagation\n",
    "            \n",
    "            classes = smooth_multi_hot(classes,len(target)).to(dev)\n",
    "\n",
    "            loss_func = nn.BCEWithLogitsLoss() # This loss function applies a multiclass loss function (B.C.E = Binary Cross Entropy)\n",
    "            loss = loss_func(y_hat,classes)#movie_ovw is already a tokenized vector\n",
    "            loss_arr.append(loss)\n",
    "            avg_loss += loss\n",
    "\n",
    "            del movie_ovw, classes\n",
    "        avg_loss = loss/batch_size\n",
    "\n",
    "        # ----------Post_Batch_BackProp----------\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        avg_loss.backward(retain_graph=False)\n",
    "        nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0)#gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the layer's squared gradient\n",
    "        l1_grad_sq.append(my_rnn.rnnL1.weight_hh.grad.norm().item()**2)\n",
    "        l2_grad_sq.append(my_rnn.rnnL2.weight_hh.grad.norm().item()**2)\n",
    "        \n",
    "        del avg_loss\n",
    "        print(f\"Current Iteration: {i*batch_size} (per batch of size {batch_size})\")\n",
    "        \n",
    "        print(torch.cuda.memory_summary(device=device))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    # Export all lists to a csv file \n",
    "    df = pd.DataFrame({\n",
    "        'l1_gradient_sq':l1_grad_sq,'l2_gradient_sq':l2_grad_sq,'loss_arr':loss_arr\n",
    "    })\n",
    "    df.to_csv(\"track.csv\", index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc887c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\orian\\AppData\\Local\\Temp\\ipykernel_22860\\3023154255.py:8: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  self.tokenized_ovw = torch.load(\"Dependencies\\overview_embs.pt\")#load the tokenized movie overviews from the SAVED file\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "class DS_Struct():\n",
    "    def __init__(self, overview_set, id_loc_set, dev):\n",
    "        self.overview_set = overview_set\n",
    "        self.id_loc_set = id_loc_set\n",
    "        self.dev = dev\n",
    "        self.tokenized_ovw = torch.load(\"Dependencies\\overview_embs.pt\")#load the tokenized movie overviews from the SAVED file\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_ovw[idx], torch.tensor(self.id_loc_set[idx])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id_loc_set)#return the length of the internal list\n",
    "\n",
    "\n",
    "def collate_fn(batch): \n",
    "    sequences, labels = zip(*batch)\n",
    "    #lengths = [len(seq) for seq in sequences]\n",
    "    \n",
    "    padded = pad_sequence(sequences, batch_first=True)\n",
    "    labels = pad_sequence(labels,batch_first=True,padding_value=pad_value) # the highest length is 9: max([len(id) for idme in movie_id_loc])) == 9\n",
    "    \n",
    "    return padded, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e419da49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overview_ds = []\\nfor i, overview in enumerate(movie_genre_ds[\"overview\"]):\\n    tokenized_ovw = my_rnn.tokenize_input(overview,device=device)\\n    overview_ds.append(tokenized_ovw.cpu())\\n    print(i)\\ntorch.save(overview_ds, \"overview_embs.pt\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete/Don't run me\n",
    "\"\"\"overview_ds = []\n",
    "for i, overview in enumerate(movie_genre_ds[\"overview\"]):\n",
    "    tokenized_ovw = my_rnn.tokenize_input(overview,device=device)\n",
    "    overview_ds.append(tokenized_ovw.cpu())\n",
    "    print(i)\n",
    "torch.save(overview_ds, \"overview_embs.pt\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c08a3",
   "metadata": {},
   "source": [
    "### **Train RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d5cebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orian\\AppData\\Local\\Temp\\ipykernel_22860\\3023154255.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return padded, torch.tensor(labels)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     13\u001b[39m train_loader = DataLoader(dataset=train_ds , batch_size=\u001b[32m16\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m0\u001b[39m, collate_fn=collate_fn)\n\u001b[32m     14\u001b[39m test_loader = DataLoader(dataset=test_ds , batch_size=\u001b[32m16\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m0\u001b[39m, collate_fn=collate_fn)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mepoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m torch.save(my_rnn.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mmodel_parameters.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#df = pd.DataFrame(test_loader)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#df.to_csv(\"test_data.csv\", index=False, header=True)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mepoch_train\u001b[39m\u001b[34m(rnn, optimizer, dev, train_ds, batch_size)\u001b[39m\n\u001b[32m     25\u001b[39m classes = torch.tensor(topK_one_hot(target,\u001b[32m19\u001b[39m)).detach()\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Get the smoothed distribution, for using a normalized probability distribution as the target\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m y_hat = \u001b[43mrnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_ovw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# Forward Propagation\u001b[39;00m\n\u001b[32m     30\u001b[39m classes = smooth_multi_hot(classes,\u001b[38;5;28mlen\u001b[39m(target)).to(dev)\n\u001b[32m     32\u001b[39m loss_func = nn.BCEWithLogitsLoss() \u001b[38;5;66;03m# This loss function applies a multiclass loss function (B.C.E = Binary Cross Entropy)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\orian\\Desktop\\Coding Project\\MovieClassifier\\Dependencies\\RNN_model_class.py:59\u001b[39m, in \u001b[36mRNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03mProcesses a batch of sequences through the RNN and attention layers.\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    x (Tensor): Input tensor of shape (batch_size, seq_len, embed_size)\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Get batch size and sequence length from the input tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m batch_size, T, _ = x.shape\n\u001b[32m     60\u001b[39m device = x.device\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# --- Initialize hidden states for the batch ---\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# These should be initialized for every new forward pass.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "import csv\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = optim.Adam(params=my_rnn.parameters())\n",
    "\n",
    "    # Loading the dataset as a class for later batching\n",
    "    gen_ds = DS_Struct(movie_genre_ds[\"overview\"], movie_id_loc, dev = device)\n",
    "\n",
    "\n",
    "    train_ds, test_ds = random_split(gen_ds, [0.8,0.2])\n",
    "    train_loader = DataLoader(dataset=train_ds , batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(dataset=test_ds , batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    epoch_train(my_rnn, optimizer=optimizer, dev=device, train_ds = train_loader)\n",
    "    torch.save(my_rnn.state_dict(), \"model_parameters.pt\")\n",
    "\n",
    "    #df = pd.DataFrame(test_loader)\n",
    "    #df.to_csv(\"test_data.csv\", index=False, header=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
