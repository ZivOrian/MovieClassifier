{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn transformers datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b67dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orian\\Desktop\\Coding Project\\MovieClassifier\\Dependencies\\MovieDataset.py:18: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  x_file_path = directory+\"\\movies_overview.csv\"\n",
      "c:\\Users\\orian\\Desktop\\Coding Project\\MovieClassifier\\Dependencies\\MovieDataset.py:19: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  label_file_path = directory+\"\\movies_genres.csv\"\n",
      "c:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from Dependencies.Early_Stop import EarlyStopping\n",
    "from Dependencies.AdditionalFunctions import topK_one_hot, smooth_multi_hot\n",
    "from Dependencies.MovieDataset import MovieGenresDataset, MovieOverviewDataset, collate_fn, PAD_VALUE, EPOCH_NUMBER\n",
    "from Dependencies.RNN_model_class import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74bf09",
   "metadata": {},
   "source": [
    "### Initialize Model and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a080ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e624f5",
   "metadata": {},
   "source": [
    "### Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e332b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_ds = MovieGenresDataset()\n",
    "movie_genre_ds = mgd_ds.getDs()\n",
    "movie_id_loc = mgd_ds.get_classes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5224125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smoothed_list(target_batch, class_lst):\n",
    "    \"\"\"\n",
    "    Create smoothed multi-hot targets from batch of genre indices.\n",
    "    Properly handles padding (values == PAD_VALUE).\n",
    "    \n",
    "    Args:\n",
    "        target_batch: Tensor of shape (batch_size, max_num_genres) with genre indices\n",
    "        class_lst: List to append results to\n",
    "        PAD_VALUE: Value used for padding (default: -1)\n",
    "    \n",
    "    Returns:\n",
    "        class_lst with smoothed targets appended\n",
    "    \"\"\"\n",
    "    for idx, target in enumerate(target_batch):\n",
    "        # Filter out padding values\n",
    "        valid_targets = target[target != PAD_VALUE]\n",
    "        \n",
    "        # ✅ CRITICAL FIX: Handle all-padding case (empty valid_targets)\n",
    "        if len(valid_targets) == 0:\n",
    "            # This sample has no valid genre labels (all padding)\n",
    "            # Use uniform distribution as fallback\n",
    "            print(f\"⚠️ Sample {idx} in batch has no valid labels (all padding), using uniform distribution\")\n",
    "            smoothed_target = torch.ones(19, dtype=torch.float32) / 19\n",
    "            class_lst.append(smoothed_target)\n",
    "            continue\n",
    "        \n",
    "        # Convert to CPU numpy for one-hot encoding\n",
    "        valid_targets_np = valid_targets.cpu().numpy()\n",
    "        \n",
    "        # ✅ Additional validation: check for invalid indices\n",
    "        if (valid_targets_np < 0).any() or (valid_targets_np >= 19).any():\n",
    "            print(f\"⚠️ Sample {idx} has invalid genre indices: {valid_targets_np}\")\n",
    "            smoothed_target = torch.ones(19, dtype=torch.float32) / 19\n",
    "            class_lst.append(smoothed_target)\n",
    "            continue\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot_target = topK_one_hot(valid_targets_np.tolist(), 19)\n",
    "        \n",
    "        # Apply smoothing (this now handles edge cases internally)\n",
    "        smoothed_target = smooth_multi_hot(\n",
    "            torch.tensor(one_hot_target, dtype=torch.float32), \n",
    "            num_valid_labels=len(valid_targets)\n",
    "        )\n",
    "        \n",
    "        # ✅ Final validation (belt and suspenders approach)\n",
    "        if torch.isnan(smoothed_target).any() or torch.isinf(smoothed_target).any():\n",
    "            print(f\"⚠️ Sample {idx} produced NaN/Inf after smoothing, using uniform distribution\")\n",
    "            smoothed_target = torch.ones(19, dtype=torch.float32) / 19\n",
    "        \n",
    "        class_lst.append(smoothed_target)\n",
    "    \n",
    "    return class_lst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def diagnose_batch(movie_ovw_batch, y_hat, classes, batch_idx):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostics to find the root cause\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DIAGNOSTICS FOR BATCH {batch_idx}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Input statistics\n",
    "    print(\"\\n1. INPUT EMBEDDINGS:\")\n",
    "    print(f\"   Shape: {movie_ovw_batch.shape}\")\n",
    "    print(f\"   Mean: {movie_ovw_batch.mean().item():.6f}\")\n",
    "    print(f\"   Std: {movie_ovw_batch.std().item():.6f}\")\n",
    "    print(f\"   Min: {movie_ovw_batch.min().item():.6f}\")\n",
    "    print(f\"   Max: {movie_ovw_batch.max().item():.6f}\")\n",
    "    print(f\"   Contains NaN: {torch.isnan(movie_ovw_batch).any().item()}\")\n",
    "    print(f\"   Contains Inf: {torch.isinf(movie_ovw_batch).any().item()}\")\n",
    "    \n",
    "    # Check for zero variance (dead features)\n",
    "    if movie_ovw_batch.std().item() < 1e-6:\n",
    "        print(f\"   ⚠️ WARNING: Input has very low variance (nearly constant)\")\n",
    "    \n",
    "    # 2. Model output (logits) statistics\n",
    "    print(\"\\n2. MODEL OUTPUT (LOGITS):\")\n",
    "    print(f\"   Shape: {y_hat.shape}\")\n",
    "    print(f\"   Mean: {y_hat.mean().item():.6f}\")\n",
    "    print(f\"   Std: {y_hat.std().item():.6f}\")\n",
    "    print(f\"   Min: {y_hat.min().item():.6f}\")\n",
    "    print(f\"   Max: {y_hat.max().item():.6f}\")\n",
    "    print(f\"   Contains NaN: {torch.isnan(y_hat).any().item()}\")\n",
    "    print(f\"   Contains Inf: {torch.isinf(y_hat).any().item()}\")\n",
    "    \n",
    "    # Check for extreme logits\n",
    "    extreme_negative = (y_hat < -50).sum().item()\n",
    "    extreme_positive = (y_hat > 50).sum().item()\n",
    "    if extreme_negative > 0:\n",
    "        print(f\"   ⚠️ WARNING: {extreme_negative} logits < -50 (will cause underflow)\")\n",
    "    if extreme_positive > 0:\n",
    "        print(f\"   ⚠️ WARNING: {extreme_positive} logits > 50 (will cause overflow)\")\n",
    "    \n",
    "    # 3. Target statistics\n",
    "    print(\"\\n3. TARGETS:\")\n",
    "    print(f\"   Shape: {classes.shape}\")\n",
    "    print(f\"   Mean: {classes.mean().item():.6f}\")\n",
    "    print(f\"   Std: {classes.std().item():.6f}\")\n",
    "    print(f\"   Min: {classes.min().item():.6f}\")\n",
    "    print(f\"   Max: {classes.max().item():.6f}\")\n",
    "    print(f\"   Contains NaN: {torch.isnan(classes).any().item()}\")\n",
    "    print(classes)\n",
    "    \n",
    "    # 4. Loss computation simulation\n",
    "    print(\"\\n4. LOSS COMPUTATION CHECK:\")\n",
    "    with torch.no_grad():\n",
    "        # Manually compute BCE with logits to see where it fails\n",
    "        sigmoid_output = torch.sigmoid(y_hat)\n",
    "        print(f\"   Sigmoid output range: [{sigmoid_output.min().item():.6f}, {sigmoid_output.max().item():.6f}]\")\n",
    "        \n",
    "        # Check for numerical issues in sigmoid\n",
    "        zeros_in_sigmoid = (sigmoid_output == 0).sum().item()\n",
    "        ones_in_sigmoid = (sigmoid_output == 1).sum().item()\n",
    "        if zeros_in_sigmoid > 0:\n",
    "            print(f\"   ⚠️ WARNING: {zeros_in_sigmoid} sigmoid outputs exactly 0 (underflow)\")\n",
    "        if ones_in_sigmoid > 0:\n",
    "            print(f\"   ⚠️ WARNING: {ones_in_sigmoid} sigmoid outputs exactly 1 (overflow)\")\n",
    "        \n",
    "        # Simulate BCE computation\n",
    "        max_val = torch.clamp(y_hat, min=0)\n",
    "        loss_part1 = (1 - classes) * y_hat\n",
    "        loss_part2 = max_val\n",
    "        loss_part3 = torch.log(torch.exp(-max_val) + torch.exp(y_hat - max_val))\n",
    "        \n",
    "        print(f\"   Loss part 1 (negative term) range: [{loss_part1.min():.6f}, {loss_part1.max():.6f}]\")\n",
    "        print(f\"   Loss part 3 (log term) contains NaN: {torch.isnan(loss_part3).any().item()}\")\n",
    "        \n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac855",
   "metadata": {},
   "source": [
    "### **Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbf3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(rnn, optimizer, dev, train_loader, val_loader, batch_size, ecpoh_num):\n",
    "    rnn.train()\n",
    "    loss_arr = []\n",
    "    l1_grad_sq = []\n",
    "    l2_grad_sq = []\n",
    "\n",
    "    i = 0\n",
    "    continue_run = True\n",
    "    enum_train = enumerate(train_loader)\n",
    "    train_size = len(train_loader) - len(train_loader) % batch_size\n",
    "\n",
    "    while i < train_size and continue_run:\n",
    "        try:\n",
    "            # ✅ NOW UNPACKING 3 VALUES: inputs, targets, and sequence lengths\n",
    "            i, (movie_ovw_batch, target_batch, seq_lengths) = next(enum_train)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        # Move batches to device\n",
    "        movie_ovw_batch = movie_ovw_batch.to(dev)\n",
    "        target_batch = target_batch.to(dev)\n",
    "        seq_lengths = seq_lengths.to(dev)\n",
    "\n",
    "        # Prepare targets\n",
    "\n",
    "        #Delete if statement later - checks if dataset contains nan/inf\n",
    "        classes_list = []\n",
    "        if torch.isnan(target_batch).any() or torch.isinf(target_batch).any():\n",
    "            print(f\"\\n{'!'*60}\")\n",
    "            print(f\"NaN/Inf VALUE DETECTED AT TARGET BATCH {i}\")\n",
    "            print(f\"{'!'*60}\")\n",
    "            print(f\"Target batch: {target_batch}\")\n",
    "            \n",
    "\n",
    "            continue_run = False\n",
    "            break\n",
    "\n",
    "        classes_list = create_smoothed_list(target_batch, classes_list)\n",
    "        classes = torch.stack(classes_list).to(dev)\n",
    "\n",
    "        # ✅ Forward Pass WITH sequence lengths\n",
    "        y_hat = rnn.forward(movie_ovw_batch, seq_lengths)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss_func = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_func(y_hat, classes)\n",
    "\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"\\n{'!'*60}\")\n",
    "            print(f\"NaN/Inf LOSS DETECTED AT BATCH {i}\")\n",
    "            print(f\"{'!'*60}\")\n",
    "            print(f\"Sequence lengths: {seq_lengths}\")\n",
    "            print(f\"Input shape: {movie_ovw_batch.shape}\")\n",
    "            print(f\"Logits range: [{y_hat.min():.4f}, {y_hat.max():.4f}]\")\n",
    "            print(f\"Target batch: {target_batch}\")\n",
    "            diagnose_batch(movie_ovw_batch,y_hat,classes,i)\n",
    "\n",
    "            continue_run = False\n",
    "            break\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Check gradients\n",
    "        if rnn.rnnL1.weight_hh.grad is not None:\n",
    "            grad_norm_l1 = rnn.rnnL1.weight_hh.grad.norm().item() ** 2\n",
    "            grad_norm_l2 = rnn.rnnL2.weight_hh.grad.norm().item() ** 2\n",
    "            \n",
    "            l1_grad_sq.append(grad_norm_l1)\n",
    "            l2_grad_sq.append(grad_norm_l2)\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        loss_arr.append(loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            avg_seq_len = seq_lengths.float().mean().item()\n",
    "            grad_info = f\"L2 grad²={grad_norm_l2:.6f}\" if rnn.rnnL1.weight_hh.grad is not None else \"No grads\"\n",
    "            print(f\"Epoch {ecpoh_num+1} | Batch {i+1}/{len(train_loader)} | \"\n",
    "                  f\"Loss={loss.item():.6f} | Avg seq len={avg_seq_len:.1f} | {grad_info}\")\n",
    "\n",
    "    print(\"\\nEpoch finished.\")\n",
    "    \n",
    "    # Save tracking data\n",
    "    if len(loss_arr) > 0:\n",
    "        df = pd.DataFrame({\n",
    "            'l1_gradient_sq': l1_grad_sq,\n",
    "            'l2_gradient_sq': l2_grad_sq,\n",
    "            'loss_arr': loss_arr\n",
    "        })\n",
    "        df.to_csv(f\"model_track_epoch_{ecpoh_num}.csv\", index=False, header=True)\n",
    "    \n",
    "    return continue_run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419da48",
   "metadata": {},
   "source": [
    "### **Dataset and DataLoader Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e419da49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from overview_embs.pt\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: This cell pre-processes all movie overviews into embeddings.\n",
    "# This should be run only ONCE to create the 'overview_embs.pt' file.\n",
    "# Running this every time would be very slow.\n",
    "# It saves the embeddings to the CPU to avoid taking up GPU memory.\n",
    "\n",
    "import os\n",
    "\n",
    "embedding_file = \"overview_embs.pt\"\n",
    "\n",
    "if not os.path.exists(embedding_file):\n",
    "    print(\"Embedding file not found. Creating embeddings...\")\n",
    "    overview_ds = []\n",
    "    # Use a temporary model on the correct device for tokenization\n",
    "    temp_model = RNN().to(device)\n",
    "    for i, overview in enumerate(movie_genre_ds[\"overview\"]):\n",
    "        # We move the embeddings to the CPU before storing them in the list\n",
    "        tokenized_ovw = temp_model.tokenize_input(overview, device=device).cpu()\n",
    "        overview_ds.append(tokenized_ovw)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{len(movie_genre_ds['overview'])} overviews\")\n",
    "    \n",
    "    torch.save(overview_ds, embedding_file)\n",
    "    print(f\"Saved embeddings to {embedding_file}\")\n",
    "    del temp_model # Free up memory\n",
    "else:\n",
    "    print(f\"Loading embeddings from {embedding_file}\")\n",
    "\n",
    "# Load the pre-computed embeddings\n",
    "tokenized_overview_tensors = torch.load(embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c08a3",
   "metadata": {},
   "source": [
    "### **Train RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d5cebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 7984, Val: 998, Test: 998\n",
      "Starting training...\n",
      "\n",
      "======================================================================\n",
      "EPOCH 1\n",
      "======================================================================\n",
      "Epoch 1 | Batch 10/1996 | Loss=0.180471 | Avg seq len=59.8 | L2 grad²=0.012565\n",
      "Epoch 1 | Batch 20/1996 | Loss=0.193750 | Avg seq len=46.2 | L2 grad²=0.017339\n",
      "Epoch 1 | Batch 30/1996 | Loss=0.171123 | Avg seq len=63.5 | L2 grad²=0.016671\n",
      "Epoch 1 | Batch 40/1996 | Loss=0.215947 | Avg seq len=106.5 | L2 grad²=0.016027\n",
      "Epoch 1 | Batch 50/1996 | Loss=0.202493 | Avg seq len=56.0 | L2 grad²=0.017212\n",
      "Epoch 1 | Batch 60/1996 | Loss=0.193738 | Avg seq len=97.2 | L2 grad²=0.017481\n",
      "Epoch 1 | Batch 70/1996 | Loss=0.167826 | Avg seq len=42.5 | L2 grad²=0.017839\n",
      "Epoch 1 | Batch 80/1996 | Loss=0.188587 | Avg seq len=32.5 | L2 grad²=0.014569\n",
      "Epoch 1 | Batch 90/1996 | Loss=0.168223 | Avg seq len=63.0 | L2 grad²=0.018745\n",
      "Epoch 1 | Batch 100/1996 | Loss=0.171668 | Avg seq len=71.0 | L2 grad²=0.018770\n",
      "Epoch 1 | Batch 110/1996 | Loss=0.178442 | Avg seq len=61.8 | L2 grad²=0.017926\n",
      "Epoch 1 | Batch 120/1996 | Loss=0.194791 | Avg seq len=66.8 | L2 grad²=0.016833\n",
      "Epoch 1 | Batch 130/1996 | Loss=0.183876 | Avg seq len=55.8 | L2 grad²=0.019799\n",
      "Epoch 1 | Batch 140/1996 | Loss=0.177960 | Avg seq len=64.5 | L2 grad²=0.016770\n",
      "Epoch 1 | Batch 150/1996 | Loss=0.176604 | Avg seq len=59.5 | L2 grad²=0.017850\n",
      "Epoch 1 | Batch 160/1996 | Loss=0.176854 | Avg seq len=50.5 | L2 grad²=0.019636\n",
      "Epoch 1 | Batch 170/1996 | Loss=0.153969 | Avg seq len=72.8 | L2 grad²=0.019098\n",
      "Epoch 1 | Batch 180/1996 | Loss=0.171196 | Avg seq len=37.0 | L2 grad²=0.018173\n",
      "Epoch 1 | Batch 190/1996 | Loss=0.171509 | Avg seq len=59.0 | L2 grad²=0.022975\n",
      "Epoch 1 | Batch 200/1996 | Loss=0.131141 | Avg seq len=70.5 | L2 grad²=0.018146\n",
      "Epoch 1 | Batch 210/1996 | Loss=0.153463 | Avg seq len=51.8 | L2 grad²=0.016059\n",
      "Epoch 1 | Batch 220/1996 | Loss=0.174978 | Avg seq len=129.5 | L2 grad²=0.018051\n",
      "Epoch 1 | Batch 230/1996 | Loss=0.134104 | Avg seq len=64.5 | L2 grad²=0.014482\n",
      "Epoch 1 | Batch 240/1996 | Loss=0.193887 | Avg seq len=76.2 | L2 grad²=0.016633\n",
      "Epoch 1 | Batch 250/1996 | Loss=0.186473 | Avg seq len=74.2 | L2 grad²=0.016540\n",
      "Epoch 1 | Batch 260/1996 | Loss=0.175020 | Avg seq len=82.2 | L2 grad²=0.016275\n",
      "Epoch 1 | Batch 270/1996 | Loss=0.155970 | Avg seq len=69.2 | L2 grad²=0.014533\n",
      "Epoch 1 | Batch 280/1996 | Loss=0.157974 | Avg seq len=83.8 | L2 grad²=0.015170\n",
      "Epoch 1 | Batch 290/1996 | Loss=0.178752 | Avg seq len=96.2 | L2 grad²=0.013470\n",
      "Epoch 1 | Batch 300/1996 | Loss=0.167785 | Avg seq len=67.8 | L2 grad²=0.014986\n",
      "Epoch 1 | Batch 310/1996 | Loss=0.157437 | Avg seq len=60.8 | L2 grad²=0.015148\n",
      "Epoch 1 | Batch 320/1996 | Loss=0.174762 | Avg seq len=95.8 | L2 grad²=0.015052\n",
      "Epoch 1 | Batch 330/1996 | Loss=0.173249 | Avg seq len=74.5 | L2 grad²=0.013745\n",
      "Epoch 1 | Batch 340/1996 | Loss=0.172700 | Avg seq len=75.8 | L2 grad²=0.011853\n",
      "Epoch 1 | Batch 350/1996 | Loss=0.163664 | Avg seq len=83.8 | L2 grad²=0.013768\n",
      "Epoch 1 | Batch 360/1996 | Loss=0.159958 | Avg seq len=44.8 | L2 grad²=0.013108\n",
      "Epoch 1 | Batch 370/1996 | Loss=0.170852 | Avg seq len=93.2 | L2 grad²=0.011598\n",
      "Epoch 1 | Batch 380/1996 | Loss=0.183691 | Avg seq len=59.8 | L2 grad²=0.012287\n",
      "Epoch 1 | Batch 390/1996 | Loss=0.173491 | Avg seq len=54.2 | L2 grad²=0.011989\n",
      "Epoch 1 | Batch 400/1996 | Loss=0.173100 | Avg seq len=66.5 | L2 grad²=0.013594\n",
      "Epoch 1 | Batch 410/1996 | Loss=0.217752 | Avg seq len=53.0 | L2 grad²=0.012267\n",
      "Epoch 1 | Batch 420/1996 | Loss=0.172364 | Avg seq len=72.5 | L2 grad²=0.013271\n",
      "Epoch 1 | Batch 430/1996 | Loss=0.183472 | Avg seq len=45.0 | L2 grad²=0.009756\n",
      "Epoch 1 | Batch 440/1996 | Loss=0.152679 | Avg seq len=70.5 | L2 grad²=0.012789\n",
      "Epoch 1 | Batch 450/1996 | Loss=0.152070 | Avg seq len=61.0 | L2 grad²=0.012821\n",
      "Epoch 1 | Batch 460/1996 | Loss=0.149597 | Avg seq len=82.5 | L2 grad²=0.013124\n",
      "Epoch 1 | Batch 470/1996 | Loss=0.149013 | Avg seq len=84.0 | L2 grad²=0.010810\n",
      "Epoch 1 | Batch 480/1996 | Loss=0.176265 | Avg seq len=67.5 | L2 grad²=0.009636\n",
      "Epoch 1 | Batch 490/1996 | Loss=0.205276 | Avg seq len=74.5 | L2 grad²=0.010500\n",
      "Epoch 1 | Batch 500/1996 | Loss=0.170684 | Avg seq len=35.8 | L2 grad²=0.011464\n",
      "Epoch 1 | Batch 510/1996 | Loss=0.171916 | Avg seq len=58.8 | L2 grad²=0.011988\n",
      "Epoch 1 | Batch 520/1996 | Loss=0.150342 | Avg seq len=61.8 | L2 grad²=0.013981\n",
      "Epoch 1 | Batch 530/1996 | Loss=0.183642 | Avg seq len=54.0 | L2 grad²=0.010289\n",
      "Epoch 1 | Batch 540/1996 | Loss=0.176311 | Avg seq len=64.2 | L2 grad²=0.015770\n",
      "Epoch 1 | Batch 550/1996 | Loss=0.175306 | Avg seq len=61.8 | L2 grad²=0.013212\n",
      "Epoch 1 | Batch 560/1996 | Loss=0.121873 | Avg seq len=98.2 | L2 grad²=0.011300\n",
      "Epoch 1 | Batch 570/1996 | Loss=0.177568 | Avg seq len=50.5 | L2 grad²=0.009834\n",
      "Epoch 1 | Batch 580/1996 | Loss=0.167838 | Avg seq len=86.2 | L2 grad²=0.008969\n",
      "Epoch 1 | Batch 590/1996 | Loss=0.155144 | Avg seq len=59.5 | L2 grad²=0.011385\n",
      "Epoch 1 | Batch 600/1996 | Loss=0.195102 | Avg seq len=80.5 | L2 grad²=0.011240\n",
      "Epoch 1 | Batch 610/1996 | Loss=0.138987 | Avg seq len=33.5 | L2 grad²=0.008823\n",
      "Epoch 1 | Batch 620/1996 | Loss=0.157763 | Avg seq len=70.0 | L2 grad²=0.011810\n",
      "Epoch 1 | Batch 630/1996 | Loss=0.154827 | Avg seq len=85.5 | L2 grad²=0.009500\n",
      "Epoch 1 | Batch 640/1996 | Loss=0.182104 | Avg seq len=109.5 | L2 grad²=0.007346\n",
      "Epoch 1 | Batch 650/1996 | Loss=0.194110 | Avg seq len=70.0 | L2 grad²=0.011167\n",
      "Epoch 1 | Batch 660/1996 | Loss=0.170841 | Avg seq len=64.5 | L2 grad²=0.014794\n",
      "Epoch 1 | Batch 670/1996 | Loss=0.161422 | Avg seq len=70.8 | L2 grad²=0.009907\n",
      "Epoch 1 | Batch 680/1996 | Loss=0.182473 | Avg seq len=76.8 | L2 grad²=0.008573\n",
      "Epoch 1 | Batch 690/1996 | Loss=0.126607 | Avg seq len=63.5 | L2 grad²=0.010781\n",
      "Epoch 1 | Batch 700/1996 | Loss=0.156504 | Avg seq len=91.5 | L2 grad²=0.009868\n",
      "Epoch 1 | Batch 710/1996 | Loss=0.170621 | Avg seq len=80.0 | L2 grad²=0.008978\n",
      "Epoch 1 | Batch 720/1996 | Loss=0.167686 | Avg seq len=47.8 | L2 grad²=0.011283\n",
      "Epoch 1 | Batch 730/1996 | Loss=0.131292 | Avg seq len=96.0 | L2 grad²=0.006737\n",
      "Epoch 1 | Batch 740/1996 | Loss=0.170072 | Avg seq len=113.5 | L2 grad²=0.007394\n",
      "Epoch 1 | Batch 750/1996 | Loss=0.142947 | Avg seq len=84.5 | L2 grad²=0.007278\n",
      "Epoch 1 | Batch 760/1996 | Loss=0.179423 | Avg seq len=95.0 | L2 grad²=0.009557\n",
      "Epoch 1 | Batch 770/1996 | Loss=0.158802 | Avg seq len=64.2 | L2 grad²=0.009302\n",
      "Epoch 1 | Batch 780/1996 | Loss=0.113985 | Avg seq len=75.8 | L2 grad²=0.010403\n",
      "Epoch 1 | Batch 790/1996 | Loss=0.188012 | Avg seq len=86.2 | L2 grad²=0.009129\n",
      "Epoch 1 | Batch 800/1996 | Loss=0.144802 | Avg seq len=85.5 | L2 grad²=0.011463\n",
      "Epoch 1 | Batch 810/1996 | Loss=0.158722 | Avg seq len=72.2 | L2 grad²=0.007717\n",
      "Epoch 1 | Batch 820/1996 | Loss=0.155779 | Avg seq len=49.5 | L2 grad²=0.008901\n",
      "Epoch 1 | Batch 830/1996 | Loss=0.162337 | Avg seq len=62.5 | L2 grad²=0.013869\n",
      "Epoch 1 | Batch 840/1996 | Loss=0.165410 | Avg seq len=52.2 | L2 grad²=0.008106\n",
      "Epoch 1 | Batch 850/1996 | Loss=0.157801 | Avg seq len=62.0 | L2 grad²=0.007322\n",
      "Epoch 1 | Batch 860/1996 | Loss=0.142974 | Avg seq len=88.8 | L2 grad²=0.012788\n",
      "Epoch 1 | Batch 870/1996 | Loss=0.137668 | Avg seq len=77.0 | L2 grad²=0.008940\n",
      "Epoch 1 | Batch 880/1996 | Loss=0.133218 | Avg seq len=50.8 | L2 grad²=0.009959\n",
      "Epoch 1 | Batch 890/1996 | Loss=0.161260 | Avg seq len=82.2 | L2 grad²=0.008067\n",
      "Epoch 1 | Batch 900/1996 | Loss=0.193048 | Avg seq len=52.0 | L2 grad²=0.006207\n",
      "Epoch 1 | Batch 910/1996 | Loss=0.178954 | Avg seq len=52.2 | L2 grad²=0.008430\n",
      "Epoch 1 | Batch 920/1996 | Loss=0.145207 | Avg seq len=58.2 | L2 grad²=0.008211\n",
      "Epoch 1 | Batch 930/1996 | Loss=0.162760 | Avg seq len=59.5 | L2 grad²=0.007395\n",
      "Epoch 1 | Batch 940/1996 | Loss=0.122265 | Avg seq len=62.5 | L2 grad²=0.011744\n",
      "Epoch 1 | Batch 950/1996 | Loss=0.211064 | Avg seq len=79.0 | L2 grad²=0.005672\n",
      "Epoch 1 | Batch 960/1996 | Loss=0.170544 | Avg seq len=78.0 | L2 grad²=0.008664\n",
      "Epoch 1 | Batch 970/1996 | Loss=0.142643 | Avg seq len=69.5 | L2 grad²=0.005478\n",
      "Epoch 1 | Batch 980/1996 | Loss=0.160092 | Avg seq len=70.8 | L2 grad²=0.007603\n",
      "Epoch 1 | Batch 990/1996 | Loss=0.119569 | Avg seq len=88.2 | L2 grad²=0.009634\n",
      "Epoch 1 | Batch 1000/1996 | Loss=0.110258 | Avg seq len=66.5 | L2 grad²=0.009136\n",
      "Epoch 1 | Batch 1010/1996 | Loss=0.129279 | Avg seq len=56.2 | L2 grad²=0.009120\n",
      "Epoch 1 | Batch 1020/1996 | Loss=0.187841 | Avg seq len=57.5 | L2 grad²=0.008188\n",
      "Epoch 1 | Batch 1030/1996 | Loss=0.174965 | Avg seq len=54.8 | L2 grad²=0.009419\n",
      "Epoch 1 | Batch 1040/1996 | Loss=0.154206 | Avg seq len=90.8 | L2 grad²=0.010059\n",
      "Epoch 1 | Batch 1050/1996 | Loss=0.168844 | Avg seq len=57.8 | L2 grad²=0.009811\n",
      "Epoch 1 | Batch 1060/1996 | Loss=0.130195 | Avg seq len=52.0 | L2 grad²=0.006356\n",
      "⚠️ Sample 3 in batch has no valid labels (all padding), using uniform distribution\n",
      "Epoch 1 | Batch 1070/1996 | Loss=0.141644 | Avg seq len=62.2 | L2 grad²=0.007118\n",
      "Epoch 1 | Batch 1080/1996 | Loss=0.154698 | Avg seq len=82.8 | L2 grad²=0.008764\n",
      "Epoch 1 | Batch 1090/1996 | Loss=0.160046 | Avg seq len=61.0 | L2 grad²=0.007218\n",
      "Epoch 1 | Batch 1100/1996 | Loss=0.172898 | Avg seq len=96.5 | L2 grad²=0.011415\n",
      "⚠️ Sample 0 in batch has no valid labels (all padding), using uniform distribution\n",
      "Epoch 1 | Batch 1110/1996 | Loss=0.176450 | Avg seq len=61.2 | L2 grad²=0.004748\n",
      "Epoch 1 | Batch 1120/1996 | Loss=0.154843 | Avg seq len=59.8 | L2 grad²=0.012188\n",
      "Epoch 1 | Batch 1130/1996 | Loss=0.137254 | Avg seq len=62.5 | L2 grad²=0.007490\n",
      "Epoch 1 | Batch 1140/1996 | Loss=0.174317 | Avg seq len=44.8 | L2 grad²=0.011028\n",
      "Epoch 1 | Batch 1150/1996 | Loss=0.167565 | Avg seq len=74.8 | L2 grad²=0.007759\n",
      "Epoch 1 | Batch 1160/1996 | Loss=0.143394 | Avg seq len=77.5 | L2 grad²=0.007155\n",
      "Epoch 1 | Batch 1170/1996 | Loss=0.144016 | Avg seq len=60.5 | L2 grad²=0.006012\n",
      "Epoch 1 | Batch 1180/1996 | Loss=0.184335 | Avg seq len=65.8 | L2 grad²=0.006547\n",
      "Epoch 1 | Batch 1190/1996 | Loss=0.154485 | Avg seq len=45.8 | L2 grad²=0.004946\n",
      "Epoch 1 | Batch 1200/1996 | Loss=0.164610 | Avg seq len=68.0 | L2 grad²=0.004807\n",
      "Epoch 1 | Batch 1210/1996 | Loss=0.170946 | Avg seq len=83.0 | L2 grad²=0.007446\n",
      "Epoch 1 | Batch 1220/1996 | Loss=0.146343 | Avg seq len=86.5 | L2 grad²=0.005410\n",
      "Epoch 1 | Batch 1230/1996 | Loss=0.166311 | Avg seq len=63.8 | L2 grad²=0.008469\n",
      "Epoch 1 | Batch 1240/1996 | Loss=0.176369 | Avg seq len=46.0 | L2 grad²=0.006261\n",
      "Epoch 1 | Batch 1250/1996 | Loss=0.169208 | Avg seq len=71.8 | L2 grad²=0.008333\n",
      "Epoch 1 | Batch 1260/1996 | Loss=0.153665 | Avg seq len=73.5 | L2 grad²=0.008301\n",
      "Epoch 1 | Batch 1270/1996 | Loss=0.140438 | Avg seq len=53.0 | L2 grad²=0.005880\n",
      "Epoch 1 | Batch 1280/1996 | Loss=0.185276 | Avg seq len=124.5 | L2 grad²=0.007469\n",
      "Epoch 1 | Batch 1290/1996 | Loss=0.164148 | Avg seq len=88.2 | L2 grad²=0.006503\n",
      "Epoch 1 | Batch 1300/1996 | Loss=0.141208 | Avg seq len=70.2 | L2 grad²=0.005136\n",
      "Epoch 1 | Batch 1310/1996 | Loss=0.169925 | Avg seq len=59.2 | L2 grad²=0.007374\n",
      "Epoch 1 | Batch 1320/1996 | Loss=0.162204 | Avg seq len=66.0 | L2 grad²=0.005648\n",
      "Epoch 1 | Batch 1330/1996 | Loss=0.142550 | Avg seq len=83.2 | L2 grad²=0.007279\n",
      "Epoch 1 | Batch 1340/1996 | Loss=0.105689 | Avg seq len=108.0 | L2 grad²=0.006718\n",
      "Epoch 1 | Batch 1350/1996 | Loss=0.169953 | Avg seq len=77.5 | L2 grad²=0.006443\n",
      "Epoch 1 | Batch 1360/1996 | Loss=0.164654 | Avg seq len=109.2 | L2 grad²=0.009098\n",
      "Epoch 1 | Batch 1370/1996 | Loss=0.094959 | Avg seq len=66.0 | L2 grad²=0.008346\n",
      "Epoch 1 | Batch 1380/1996 | Loss=0.143537 | Avg seq len=74.8 | L2 grad²=0.004691\n",
      "Epoch 1 | Batch 1390/1996 | Loss=0.194549 | Avg seq len=63.2 | L2 grad²=0.007045\n",
      "Epoch 1 | Batch 1400/1996 | Loss=0.121408 | Avg seq len=57.0 | L2 grad²=0.007601\n",
      "Epoch 1 | Batch 1410/1996 | Loss=0.158936 | Avg seq len=81.2 | L2 grad²=0.008071\n",
      "Epoch 1 | Batch 1420/1996 | Loss=0.135734 | Avg seq len=88.8 | L2 grad²=0.006084\n",
      "Epoch 1 | Batch 1430/1996 | Loss=0.162199 | Avg seq len=59.0 | L2 grad²=0.005179\n",
      "Epoch 1 | Batch 1440/1996 | Loss=0.166810 | Avg seq len=114.2 | L2 grad²=0.005913\n",
      "Epoch 1 | Batch 1450/1996 | Loss=0.186450 | Avg seq len=90.8 | L2 grad²=0.007539\n",
      "Epoch 1 | Batch 1460/1996 | Loss=0.136839 | Avg seq len=74.8 | L2 grad²=0.006673\n",
      "Epoch 1 | Batch 1470/1996 | Loss=0.150661 | Avg seq len=61.5 | L2 grad²=0.006214\n",
      "Epoch 1 | Batch 1480/1996 | Loss=0.160829 | Avg seq len=51.0 | L2 grad²=0.007500\n",
      "Epoch 1 | Batch 1490/1996 | Loss=0.161583 | Avg seq len=45.0 | L2 grad²=0.007064\n",
      "Epoch 1 | Batch 1500/1996 | Loss=0.156012 | Avg seq len=85.8 | L2 grad²=0.003945\n",
      "Epoch 1 | Batch 1510/1996 | Loss=0.136336 | Avg seq len=74.0 | L2 grad²=0.007077\n",
      "Epoch 1 | Batch 1520/1996 | Loss=0.130296 | Avg seq len=72.2 | L2 grad²=0.008149\n",
      "Epoch 1 | Batch 1530/1996 | Loss=0.162049 | Avg seq len=58.0 | L2 grad²=0.007393\n",
      "Epoch 1 | Batch 1540/1996 | Loss=0.148900 | Avg seq len=92.5 | L2 grad²=0.005983\n",
      "Epoch 1 | Batch 1550/1996 | Loss=0.141830 | Avg seq len=75.5 | L2 grad²=0.005275\n",
      "Epoch 1 | Batch 1560/1996 | Loss=0.143400 | Avg seq len=36.8 | L2 grad²=0.006997\n",
      "Epoch 1 | Batch 1570/1996 | Loss=0.156254 | Avg seq len=56.0 | L2 grad²=0.008531\n",
      "Epoch 1 | Batch 1580/1996 | Loss=0.135175 | Avg seq len=81.2 | L2 grad²=0.004298\n",
      "Epoch 1 | Batch 1590/1996 | Loss=0.166060 | Avg seq len=84.8 | L2 grad²=0.007473\n",
      "⚠️ Sample 3 in batch has no valid labels (all padding), using uniform distribution\n",
      "Epoch 1 | Batch 1600/1996 | Loss=0.149592 | Avg seq len=56.2 | L2 grad²=0.007209\n",
      "Epoch 1 | Batch 1610/1996 | Loss=0.154694 | Avg seq len=57.2 | L2 grad²=0.005491\n",
      "Epoch 1 | Batch 1620/1996 | Loss=0.174414 | Avg seq len=44.2 | L2 grad²=0.005416\n",
      "Epoch 1 | Batch 1630/1996 | Loss=0.147051 | Avg seq len=63.0 | L2 grad²=0.005311\n",
      "Epoch 1 | Batch 1640/1996 | Loss=0.162168 | Avg seq len=41.2 | L2 grad²=0.005398\n",
      "Epoch 1 | Batch 1650/1996 | Loss=0.133959 | Avg seq len=34.0 | L2 grad²=0.007059\n",
      "Epoch 1 | Batch 1660/1996 | Loss=0.129001 | Avg seq len=74.0 | L2 grad²=0.006782\n",
      "Epoch 1 | Batch 1670/1996 | Loss=0.137248 | Avg seq len=91.8 | L2 grad²=0.008060\n",
      "Epoch 1 | Batch 1680/1996 | Loss=0.157914 | Avg seq len=76.5 | L2 grad²=0.008519\n",
      "Epoch 1 | Batch 1690/1996 | Loss=0.185352 | Avg seq len=77.0 | L2 grad²=0.004893\n",
      "Epoch 1 | Batch 1700/1996 | Loss=0.185677 | Avg seq len=82.8 | L2 grad²=0.003638\n",
      "Epoch 1 | Batch 1710/1996 | Loss=0.157715 | Avg seq len=104.8 | L2 grad²=0.004889\n",
      "Epoch 1 | Batch 1720/1996 | Loss=0.153898 | Avg seq len=66.5 | L2 grad²=0.003591\n",
      "Epoch 1 | Batch 1730/1996 | Loss=0.158392 | Avg seq len=83.5 | L2 grad²=0.004396\n",
      "Epoch 1 | Batch 1740/1996 | Loss=0.168926 | Avg seq len=51.2 | L2 grad²=0.004868\n",
      "Epoch 1 | Batch 1750/1996 | Loss=0.176244 | Avg seq len=105.0 | L2 grad²=0.004022\n",
      "Epoch 1 | Batch 1760/1996 | Loss=0.147606 | Avg seq len=71.8 | L2 grad²=0.004787\n",
      "Epoch 1 | Batch 1770/1996 | Loss=0.099584 | Avg seq len=60.5 | L2 grad²=0.006216\n",
      "Epoch 1 | Batch 1780/1996 | Loss=0.153797 | Avg seq len=76.0 | L2 grad²=0.004544\n",
      "Epoch 1 | Batch 1790/1996 | Loss=0.126145 | Avg seq len=55.8 | L2 grad²=0.002895\n",
      "Epoch 1 | Batch 1800/1996 | Loss=0.162106 | Avg seq len=87.5 | L2 grad²=0.009043\n",
      "Epoch 1 | Batch 1810/1996 | Loss=0.128454 | Avg seq len=85.2 | L2 grad²=0.007410\n",
      "Epoch 1 | Batch 1820/1996 | Loss=0.190057 | Avg seq len=87.8 | L2 grad²=0.007651\n",
      "Epoch 1 | Batch 1830/1996 | Loss=0.148973 | Avg seq len=45.2 | L2 grad²=0.006458\n",
      "Epoch 1 | Batch 1840/1996 | Loss=0.160363 | Avg seq len=49.5 | L2 grad²=0.003866\n",
      "Epoch 1 | Batch 1850/1996 | Loss=0.166953 | Avg seq len=89.8 | L2 grad²=0.004391\n",
      "Epoch 1 | Batch 1860/1996 | Loss=0.161062 | Avg seq len=77.8 | L2 grad²=0.004260\n",
      "Epoch 1 | Batch 1870/1996 | Loss=0.156355 | Avg seq len=76.0 | L2 grad²=0.004171\n",
      "Epoch 1 | Batch 1880/1996 | Loss=0.141665 | Avg seq len=43.2 | L2 grad²=0.006738\n",
      "Epoch 1 | Batch 1890/1996 | Loss=0.147563 | Avg seq len=94.8 | L2 grad²=0.004434\n",
      "Epoch 1 | Batch 1900/1996 | Loss=0.136543 | Avg seq len=66.2 | L2 grad²=0.005494\n",
      "Epoch 1 | Batch 1910/1996 | Loss=0.183614 | Avg seq len=55.2 | L2 grad²=0.004593\n",
      "Epoch 1 | Batch 1920/1996 | Loss=0.140063 | Avg seq len=75.5 | L2 grad²=0.004371\n",
      "Epoch 1 | Batch 1930/1996 | Loss=0.142775 | Avg seq len=71.2 | L2 grad²=0.004949\n",
      "Epoch 1 | Batch 1940/1996 | Loss=0.157643 | Avg seq len=118.8 | L2 grad²=0.007231\n",
      "Epoch 1 | Batch 1950/1996 | Loss=0.154827 | Avg seq len=59.2 | L2 grad²=0.005285\n",
      "Epoch 1 | Batch 1960/1996 | Loss=0.149937 | Avg seq len=67.5 | L2 grad²=0.006355\n",
      "Epoch 1 | Batch 1970/1996 | Loss=0.174081 | Avg seq len=65.2 | L2 grad²=0.004565\n",
      "Epoch 1 | Batch 1980/1996 | Loss=0.196120 | Avg seq len=41.5 | L2 grad²=0.004676\n",
      "Epoch 1 | Batch 1990/1996 | Loss=0.170709 | Avg seq len=63.0 | L2 grad²=0.003322\n",
      "\n",
      "Epoch finished.\n",
      "\n",
      "Training complete. Saving model...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 4\n",
    "    \n",
    "    my_rnn = RNN().to(device)\n",
    "    optimizer = optim.Adam(params=my_rnn.parameters(), lr=1e-5, weight_decay=5e-3)\n",
    "    \n",
    "    full_dataset = MovieOverviewDataset(tokenized_overview_tensors, movie_id_loc)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = int((len(full_dataset) - train_size) / 2) \n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "    train_ds, test_ds, val_ds = random_split(full_dataset, [train_size, test_size, val_size])\n",
    "    \n",
    "    # ✅ Use the NEW collate_fn that returns sequence lengths\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                             num_workers=0, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(dataset=test_ds, batch_size=1, shuffle=True, \n",
    "                            num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(dataset=val_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                           num_workers=0, collate_fn=collate_fn)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch_iter in range(EPOCH_NUMBER):\n",
    "        print(f\"\\n{'='*70}\\nEPOCH {epoch_iter + 1}\\n{'='*70}\")\n",
    "        continue_run = epoch_train(my_rnn, optimizer=optimizer, dev=device, \n",
    "                                   train_loader=train_loader, val_loader=val_loader, \n",
    "                                   batch_size=BATCH_SIZE, ecpoh_num=epoch_iter)\n",
    "        \n",
    "        if not continue_run:\n",
    "            print(f\"Training stopped at epoch {epoch_iter + 1}\")\n",
    "            break\n",
    "    \n",
    "    if continue_run:\n",
    "        print(\"\\nTraining complete. Saving model...\")\n",
    "        torch.save(my_rnn.state_dict(), \"model_parameters.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
