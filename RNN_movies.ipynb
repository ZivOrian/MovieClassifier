{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn transformers datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b67dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from Dependencies.Early_Stop import EarlyStopping\n",
    "from Dependencies.AdditionalFunctions import topK_one_hot, smooth_multi_hot\n",
    "from Dependencies.MovieDataset import MovieGenresDataset\n",
    "from Dependencies.RNN_model_class import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74bf09",
   "metadata": {},
   "source": [
    "### Initialize Model and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a080ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e624f5",
   "metadata": {},
   "source": [
    "### Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e332b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_ds = MovieGenresDataset()\n",
    "movie_genre_ds = mgd_ds.getDs()\n",
    "movie_id_loc = mgd_ds.get_classes()\n",
    "\n",
    "# This value will be used for padding label sequences\n",
    "pad_value = -1 # Using -1 is safer than a magic number like 5555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5224125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smoothed_list(target_batch,class_lst):\n",
    "    for target in target_batch:\n",
    "            # Filter out padding values from the target tensor\n",
    "            valid_targets = target[target != pad_value]\n",
    "            one_hot_target = topK_one_hot(valid_targets.cpu().numpy(), 19) # Moving targets to cpu to save memory\n",
    "            smoothed_target = smooth_multi_hot(torch.tensor(one_hot_target), len(valid_targets))\n",
    "            class_lst.append(smoothed_target)\n",
    "    return class_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac855",
   "metadata": {},
   "source": [
    "### **Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbf3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(rnn, optimizer, dev, train_loader, val_loader, batch_size):\n",
    "    rnn.train() # Set the model to training mode\n",
    "    loss_arr = []\n",
    "    l1_grad_sq = []\n",
    "    l2_grad_sq = []\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    continue_run = True\n",
    "    enum_train = enumerate(train_loader)\n",
    "\n",
    "\n",
    "    # Loop that processes the entire batch at once with early stopping\n",
    "    while i-1<len(train_loader) and continue_run:\n",
    "        i, (movie_ovw_batch, target_batch) = next(enum_train)\n",
    "        # Setting up an early stopping class\n",
    "        es = EarlyStopping()\n",
    "        # batching and loading them onto de-facto device (GPU / CPU)\n",
    "        movie_ovw_batch = movie_ovw_batch.to(dev)\n",
    "        target_batch = target_batch.to(dev)\n",
    "\n",
    "        # --- Prepare targets for the entire batch ---\n",
    "        # These lists will hold the processed multi-hot encoded targets for each item in the batch\n",
    "        classes_list = []\n",
    "        classes_list = create_smoothed_list(target_batch ,classes_list)\n",
    "        \n",
    "        # Stack the list of tensors into a single batch tensor\n",
    "        classes = torch.stack(classes_list).to(dev)\n",
    "\n",
    "        # --- Forward Pass ---\n",
    "        y_hat = rnn.forward(movie_ovw_batch)\n",
    "\n",
    "        # --- Loss Calculation ---\n",
    "        loss_func = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_func(y_hat, classes)\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1e-4) # Gradient clipping\n",
    "        \n",
    "        # Store the layer's squared gradient norm *before* the optimizer step\n",
    "        if rnn.rnnL1.weight_hh.grad is not None:\n",
    "            l1_grad_sq.append(rnn.rnnL1.weight_hh.grad.norm().item()**2)\n",
    "            l2_grad_sq.append(rnn.rnnL2.weight_hh.grad.norm().item()**2)\n",
    "        \n",
    "        # --- Optimizer step ---\n",
    "        \n",
    "        if not math.isnan(rnn.rnnL2.weight_hh.grad.norm().item()**2):\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(f\"These are the logits{y_hat}\\n\\nThis is the input{movie_ovw_batch}\")\n",
    "            continue_run=False\n",
    "\n",
    "        \n",
    "        loss_arr.append(loss.item())\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if (i + 1) % 10 == 0: \n",
    "            print(f\"Batch {i+1}/{len(train_loader)},gradient = {rnn.rnnL2.weight_hh.grad.norm().item()**2}, Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            print(rnn.rnnL2.weight_hh.grad.norm() ,math.isnan(rnn.rnnL2.weight_hh.grad.norm().item()**2))\n",
    "            # For debugging memory, uncomment the line below:\n",
    "            # print(torch.cuda.memory_summary(device=dev))\n",
    "\n",
    "\n",
    "        # ----- Validation Section ----- \n",
    "        #     (done per step count)\n",
    "        \n",
    "        # Validate model with a validation batch every 50 batches\n",
    "        \"\"\"if i%200==0 and i!=0 and j<len(val_loader.dataset):\n",
    "            rnn.eval()#setting model to evaluation mode\n",
    "\n",
    "            j, (val_movie_ovw_batch, val_target_batch) = next(enumerate(val_loader))\n",
    "            val_movie_ovw_batch = val_movie_ovw_batch.to(dev)\n",
    "            val_target_batch = val_target_batch.to(dev)\n",
    "\n",
    "            val_class_lst = []\n",
    "            val_class_lst = create_smoothed_list(val_target_batch, val_class_lst)\n",
    "            valuation_classes = torch.stack(val_class_lst).to(dev)\n",
    "            with torch.no_grad():\n",
    "                print(f\"yep. that's the shape - {val_movie_ovw_batch.shape}\")\n",
    "                val_y_hat = rnn.forward(val_movie_ovw_batch)\n",
    "                vloss = loss_func(val_y_hat, valuation_classes)\n",
    "                # Set the continue boolean to false if the model worsens\n",
    "                continue_run = es(rnn, vloss)\n",
    "\n",
    "            rnn.train()# set the model back to training mode\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Epoch finished.\")\n",
    "    # Save tracking data\n",
    "    df = pd.DataFrame({\n",
    "        'l1_gradient_sq': l1_grad_sq,\n",
    "        'l2_gradient_sq': l2_grad_sq,\n",
    "        'loss_arr': loss_arr\n",
    "    })\n",
    "    df.to_csv(\"track.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419da48",
   "metadata": {},
   "source": [
    "### **Dataset and DataLoader Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc887c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MovieOverviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_overviews, id_loc_set):\n",
    "        self.tokenized_ovw = tokenized_overviews\n",
    "        self.id_loc_set = id_loc_set\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_ovw[idx], torch.tensor(self.id_loc_set[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_loc_set)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences (features)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad labels (targets)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=pad_value)\n",
    "    \n",
    "    return padded_sequences, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e419da49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from overview_embs.pt\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: This cell pre-processes all movie overviews into embeddings.\n",
    "# This should be run only ONCE to create the 'overview_embs.pt' file.\n",
    "# Running this every time would be very slow.\n",
    "# It saves the embeddings to the CPU to avoid taking up GPU memory.\n",
    "\n",
    "import os\n",
    "\n",
    "embedding_file = \"overview_embs.pt\"\n",
    "\n",
    "if not os.path.exists(embedding_file):\n",
    "    print(\"Embedding file not found. Creating embeddings...\")\n",
    "    overview_ds = []\n",
    "    # Use a temporary model on the correct device for tokenization\n",
    "    temp_model = RNN().to(device)\n",
    "    for i, overview in enumerate(movie_genre_ds[\"overview\"]):\n",
    "        # We move the embeddings to the CPU before storing them in the list\n",
    "        tokenized_ovw = temp_model.tokenize_input(overview, device=device).cpu()\n",
    "        overview_ds.append(tokenized_ovw)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{len(movie_genre_ds['overview'])} overviews\")\n",
    "    \n",
    "    torch.save(overview_ds, embedding_file)\n",
    "    print(f\"Saved embeddings to {embedding_file}\")\n",
    "    del temp_model # Free up memory\n",
    "else:\n",
    "    print(f\"Loading embeddings from {embedding_file}\")\n",
    "\n",
    "# Load the pre-computed embeddings\n",
    "tokenized_overview_tensors = torch.load(embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c08a3",
   "metadata": {},
   "source": [
    "### **Train RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d5cebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n",
      "Starting training...\n",
      "Batch 10/1996,gradient = 1.6437709909062587e-11, Loss = 0.6864\n",
      "tensor(4.0543e-06, device='cuda:0') False\n",
      "Batch 20/1996,gradient = 2.2156496659322102e-13, Loss = 0.6883\n",
      "tensor(4.7071e-07, device='cuda:0') False\n",
      "Batch 30/1996,gradient = 2.9574146045585677e-15, Loss = 0.6874\n",
      "tensor(5.4382e-08, device='cuda:0') False\n",
      "Batch 40/1996,gradient = 1.0046701407639571e-16, Loss = 0.6853\n",
      "tensor(1.0023e-08, device='cuda:0') False\n",
      "Batch 50/1996,gradient = 1.395101684817269e-17, Loss = 0.6838\n",
      "tensor(3.7351e-09, device='cuda:0') False\n",
      "Batch 60/1996,gradient = 2.4695233998283106e-18, Loss = 0.6832\n",
      "tensor(1.5715e-09, device='cuda:0') False\n",
      "Batch 70/1996,gradient = 2.304757171478964e-19, Loss = 0.6830\n",
      "tensor(4.8008e-10, device='cuda:0') False\n",
      "Batch 80/1996,gradient = 4.1742689300050775e-20, Loss = 0.6829\n",
      "tensor(2.0431e-10, device='cuda:0') False\n",
      "Batch 90/1996,gradient = 2.520590087811177e-21, Loss = 0.6829\n",
      "tensor(5.0205e-11, device='cuda:0') False\n",
      "Batch 100/1996,gradient = 5.881177569194794e-22, Loss = 0.6829\n",
      "tensor(2.4251e-11, device='cuda:0') False\n",
      "Batch 110/1996,gradient = 6.781023103598993e-23, Loss = 0.6829\n",
      "tensor(8.2347e-12, device='cuda:0') False\n",
      "Batch 120/1996,gradient = 1.0047436367106162e-23, Loss = 0.6829\n",
      "tensor(3.1698e-12, device='cuda:0') False\n",
      "Batch 130/1996,gradient = 9.672747242212825e-25, Loss = 0.6829\n",
      "tensor(9.8350e-13, device='cuda:0') False\n",
      "Batch 140/1996,gradient = 1.2469309245860711e-25, Loss = 0.6830\n",
      "tensor(3.5312e-13, device='cuda:0') False\n",
      "Batch 150/1996,gradient = 1.942699310710075e-26, Loss = 0.6829\n",
      "tensor(1.3938e-13, device='cuda:0') False\n",
      "Batch 160/1996,gradient = 2.1942475015010387e-27, Loss = 0.6829\n",
      "tensor(4.6843e-14, device='cuda:0') False\n",
      "Batch 170/1996,gradient = 2.6061957075079645e-28, Loss = 0.6830\n",
      "tensor(1.6144e-14, device='cuda:0') False\n",
      "Batch 180/1996,gradient = 4.2470984376213127e-29, Loss = 0.6829\n",
      "tensor(6.5170e-15, device='cuda:0') False\n",
      "Batch 190/1996,gradient = 4.931120776648068e-30, Loss = 0.6829\n",
      "tensor(2.2206e-15, device='cuda:0') False\n",
      "Batch 200/1996,gradient = 5.29639647250501e-31, Loss = 0.6830\n",
      "tensor(7.2776e-16, device='cuda:0') False\n",
      "Batch 210/1996,gradient = 7.284068467528368e-32, Loss = 0.6829\n",
      "tensor(2.6989e-16, device='cuda:0') False\n",
      "Batch 220/1996,gradient = 9.416250203595709e-33, Loss = 0.6830\n",
      "tensor(9.7037e-17, device='cuda:0') False\n",
      "Batch 230/1996,gradient = 1.0173810974254568e-33, Loss = 0.6829\n",
      "tensor(3.1896e-17, device='cuda:0') False\n",
      "Batch 240/1996,gradient = 1.3221260060758417e-34, Loss = 0.6830\n",
      "tensor(1.1498e-17, device='cuda:0') False\n",
      "Batch 250/1996,gradient = 1.8160051742645455e-35, Loss = 0.6830\n",
      "tensor(4.2615e-18, device='cuda:0') False\n",
      "Batch 260/1996,gradient = 2.1551002510284024e-36, Loss = 0.6829\n",
      "tensor(1.4680e-18, device='cuda:0') False\n",
      "Batch 270/1996,gradient = 2.3482712704082584e-37, Loss = 0.6829\n",
      "tensor(4.8459e-19, device='cuda:0') False\n",
      "Batch 280/1996,gradient = 3.802904154088538e-38, Loss = 0.6830\n",
      "tensor(1.9501e-19, device='cuda:0') False\n",
      "Batch 290/1996,gradient = 3.780215368057109e-39, Loss = 0.6830\n",
      "tensor(6.1483e-20, device='cuda:0') False\n",
      "Batch 300/1996,gradient = 4.6257980472887595e-40, Loss = 0.6829\n",
      "tensor(2.1508e-20, device='cuda:0') False\n",
      "Batch 310/1996,gradient = 2.5390126003686264e-41, Loss = 0.6829\n",
      "tensor(5.0389e-21, device='cuda:0') False\n",
      "Batch 320/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 330/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 340/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 350/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 360/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 370/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 380/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 390/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 400/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 410/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 420/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 430/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 440/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 450/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 460/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 470/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 480/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 490/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 500/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 510/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 520/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 530/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 540/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 550/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 560/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 570/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 580/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 590/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 600/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 610/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 620/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 630/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 640/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 650/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 660/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 670/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 680/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 690/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 700/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 710/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 720/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 730/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 740/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 750/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 760/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 770/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 780/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 790/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 800/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 810/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 820/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 830/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 840/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 850/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 860/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 870/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 880/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 890/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 900/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 910/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 920/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 930/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 940/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 950/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 960/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 970/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 980/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 990/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1000/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1010/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1020/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1030/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1040/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1050/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1060/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1070/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1080/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1090/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1100/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1110/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1120/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1130/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1140/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1150/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1160/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1170/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1180/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1190/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1200/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1210/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1220/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1230/1996,gradient = 0.0, Loss = 0.6830\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1240/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "Batch 1250/1996,gradient = 0.0, Loss = 0.6829\n",
      "tensor(0., device='cuda:0') False\n",
      "These are the logitstensor([[-0.0227, -0.0228, -0.0231, -0.0223, -0.0230, -0.0231, -0.0221, -0.0232,\n",
      "         -0.0230, -0.0231, -0.0231, -0.0230, -0.0231, -0.0228, -0.0230, -0.0231,\n",
      "         -0.0223, -0.0230, -0.0231],\n",
      "        [-0.0227, -0.0228, -0.0231, -0.0223, -0.0230, -0.0231, -0.0221, -0.0232,\n",
      "         -0.0230, -0.0231, -0.0231, -0.0230, -0.0231, -0.0228, -0.0230, -0.0231,\n",
      "         -0.0223, -0.0230, -0.0231],\n",
      "        [-0.0227, -0.0228, -0.0231, -0.0223, -0.0230, -0.0231, -0.0221, -0.0232,\n",
      "         -0.0230, -0.0231, -0.0231, -0.0230, -0.0231, -0.0228, -0.0230, -0.0231,\n",
      "         -0.0223, -0.0230, -0.0231],\n",
      "        [-0.0227, -0.0228, -0.0231, -0.0223, -0.0230, -0.0231, -0.0221, -0.0232,\n",
      "         -0.0230, -0.0231, -0.0231, -0.0230, -0.0231, -0.0228, -0.0230, -0.0231,\n",
      "         -0.0223, -0.0230, -0.0231]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "This is the inputtensor([[[-0.3890, -0.1079,  0.1835,  ...,  0.1020,  0.4537,  0.6747],\n",
      "         [-0.3472, -0.1249,  0.2729,  ...,  0.3007,  0.2349,  0.1663],\n",
      "         [-0.2379, -0.4489, -0.1168,  ...,  0.2307, -0.2158, -0.1349],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.3540, -0.0636,  0.1250,  ...,  0.2331,  0.4645,  0.3636],\n",
      "         [-0.0832,  0.2273,  0.5392,  ...,  0.6599,  0.5475,  0.2198],\n",
      "         [ 0.1624, -0.2336,  0.1672,  ...,  0.3075, -0.6995, -0.7553],\n",
      "         ...,\n",
      "         [ 0.0016,  0.0248,  1.1395,  ...,  0.3248,  0.3113, -0.5215],\n",
      "         [ 0.6933,  0.0273, -0.1477,  ...,  0.2152, -0.3947, -0.3327],\n",
      "         [ 0.0106,  0.4768,  0.1242,  ...,  0.7034, -0.4869, -0.6871]],\n",
      "\n",
      "        [[ 0.0851, -0.1937,  0.3227,  ..., -0.3308,  0.5226,  0.4056],\n",
      "         [-0.3713,  0.1367,  0.5812,  ..., -0.0101,  0.4103,  0.0058],\n",
      "         [ 0.2728, -0.1625,  0.4172,  ...,  0.1968, -0.3876, -0.3867],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2809, -0.1766, -0.0668,  ...,  0.1690,  0.4489,  0.3944],\n",
      "         [-0.3579,  0.2242,  0.7883,  ...,  0.2074,  0.0765, -0.0212],\n",
      "         [ 0.0928, -0.1332,  0.0311,  ...,  0.2453, -0.4962, -0.4099],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Epoch finished.\n",
      "Training complete. Saving model...\n",
      "Model saved to model_parameters.pt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 4 # Defining batch size as a variable\n",
    "\n",
    "    my_rnn = RNN().to(device)\n",
    "    optimizer = optim.Adam(params=my_rnn.parameters(), lr=0.001, weight_decay=1.e-3)\n",
    "\n",
    "    # Create the dataset instance with the pre-loaded embeddings\n",
    "    full_dataset = MovieOverviewDataset(tokenized_overview_tensors, movie_id_loc)\n",
    "\n",
    "    # Split dataset into train, test and validation datasets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = int((len(full_dataset) - train_size)/2) \n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "\n",
    "    print(val_size)\n",
    "    train_ds, test_ds, val_ds = random_split(full_dataset, [train_size, test_size, val_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(dataset=test_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(dataset=test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    np.set_printoptions(threshold=None)\n",
    "    print(\"Starting training...\")\n",
    "    epoch_train(my_rnn, optimizer=optimizer, dev=device, train_loader=train_loader, val_loader=val_loader, batch_size=BATCH_SIZE)\n",
    "\n",
    "    \n",
    "    print(\"Training complete. Saving model...\")\n",
    "    torch.save(my_rnn.state_dict(), \"model_parameters.pt\")\n",
    "    print(\"Model saved to model_parameters.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
